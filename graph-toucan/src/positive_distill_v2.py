"""
æ­£å‘è’¸é¦ V2ï¼šåŸºäº fsp_v2_queries.jsonl çš„å¤šè½®è’¸é¦

å®ç° MAGNET çš„ Context Distillationï¼š
1. ä¸ºæ¯ä¸ª turn æ„å»º hintsï¼ˆæ ¹æ® turn_type ä¸åŒé‡‡ç”¨ä¸åŒç­–ç•¥ï¼‰
2. ä½¿ç”¨æ•™å¸ˆæ¨¡å‹é€è½®ç”Ÿæˆ reasoning + function calls
3. æ‰§è¡Œå‡½æ•°è·å– outputsï¼ˆæˆ–ä½¿ç”¨ ground truthï¼‰
4. ç”Ÿæˆå®Œæ•´çš„å¯¹è¯å†å²ä½œä¸º SFT è®­ç»ƒæ•°æ®
"""

import asyncio
import json
import os
import sys
import yaml
import hashlib
import copy
from typing import Any, Dict, List, Optional, Tuple
from tqdm import tqdm
from openai import AsyncOpenAI

# å¯¼å…¥ backward_to_query ä¸­çš„å‡½æ•°æ‰§è¡Œé€»è¾‘
sys.path.insert(0, os.path.dirname(__file__))
from backward_to_query import execute_function_call

# è·¯å¾„é…ç½®
ROOT_DIR = "/data/lhy/datasets/graph-Toucan"
FSP_V2_PATH = os.path.join(ROOT_DIR, "fsp_path/fsp_v2_queries.jsonl")
DISTILL_V2_OUTPUT = os.path.join(ROOT_DIR, "distill/distill_v3.jsonl")
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
TOOL_SCHEMA_PATH = os.path.join(ROOT_DIR, "tool_info/tool_schema_with_outputformat.json")


def load_config(config_path: str = CONFIG_PATH) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


def load_all_tool_schemas(schema_path: str = TOOL_SCHEMA_PATH) -> Dict[str, Dict[str, Any]]:
    """
    åŠ è½½æ‰€æœ‰ tool schemasï¼ˆä¸åŒ…å« output formatï¼‰

    Args:
        schema_path: tool schema æ–‡ä»¶è·¯å¾„

    Returns:
        {tool_name: {"function_schema": {...}}} æ ¼å¼çš„å­—å…¸
    """
    with open(schema_path, "r", encoding="utf-8") as f:
        all_schemas = json.load(f)

    # åªä¿ç•™ function_schemaï¼Œç§»é™¤ output_schema
    tool_schemas = {}
    for tool_name, tool_data in all_schemas.items():
        if "function_schema" in tool_data:
            tool_schemas[tool_name] = {
                "function_schema": tool_data["function_schema"]
            }

    return tool_schemas


def extract_tool_schemas_for_path(path_data: Dict, all_tool_schemas: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    ä» path_data ä¸­æå–éœ€è¦çš„ tool schemas

    Args:
        path_data: path æ•°æ®
        all_tool_schemas: æ‰€æœ‰çš„ tool schemas

    Returns:
        è¯¥ path éœ€è¦çš„ tool schemas
    """
    # æ”¶é›†è¯¥ path ä¸­æ‰€æœ‰å‡ºç°çš„å‡½æ•°å
    function_names = set()
    for turn_data in path_data.get('turns_data', []):
        functions = turn_data.get('functions', [])
        function_names.update(functions)

    # æå–å¯¹åº”çš„ schemas
    path_tool_schemas = {}
    for func_name in function_names:
        if func_name in all_tool_schemas:
            path_tool_schemas[func_name] = all_tool_schemas[func_name]
        else:
            print(f"    Warning: Tool schema not found for function: {func_name}")

    return path_tool_schemas


# åŠ è½½é…ç½®
config = load_config()

# åˆå§‹åŒ– AsyncOpenAI å®¢æˆ·ç«¯
api_key_env = config["api"].get("api_key_env")
if api_key_env:
    api_key = os.getenv(api_key_env, "EMPTY")
else:
    api_key = config["api"].get("api_key", "EMPTY")
base_url = config["api"]["base_url"]

async_client = AsyncOpenAI(
    api_key=api_key,
    base_url=base_url,
)

# æ¨¡å‹é…ç½®
TEACHER_MODEL = config["model"].get("teacher", config["model"]["default"])

# ç³»ç»Ÿæç¤º
SYSTEM_PROMPT = """You are an expert AI assistant specialized in multi-turn function calling.

Your task is to help users accomplish their goals by:
1. Understanding their queries and conversation history
2. Reasoning about which functions to call
3. Executing the appropriate tool calls
4. Summarizing the results

IMPORTANT Instructions:
- Think step-by-step about the user's request
- Explain your reasoning clearly before making function calls
- Use pronouns (e.g., "that result", "those coordinates") to reference previous outputs
- When you see [Hint] in the messages, use them to guide your reasoning but DO NOT explicitly mention the hints in your response
- Always generate responses in English

Response Format:
1. First, provide your reasoning (1-2 paragraphs explaining your understanding and approach)
2. Then, make the appropriate function calls
3. After receiving tool outputs, summarize the results for the user
"""


def shorten_tool_name(name: str, max_length: int = 64) -> str:
    """ç¼©çŸ­å·¥å…·åç§°åˆ°æŒ‡å®šé•¿åº¦"""
    if len(name) <= max_length:
        return name
    hash_suffix = hashlib.md5(name.encode()).hexdigest()[:5]
    max_prefix = max_length - 6
    return f"{name[:max_prefix]}_{hash_suffix}"


def build_tools_for_api(
    tool_schemas: Dict[str, Dict[str, Any]]
) -> Tuple[List[Dict[str, Any]], Dict[str, str], Dict[str, Dict[str, Any]]]:
    """
    å°† tool schemas è½¬æ¢ä¸º OpenAI API æ ¼å¼

    Returns:
        - tools: OpenAI API æ ¼å¼çš„ tools åˆ—è¡¨ï¼ˆåç§°å·²ç¼©çŸ­ï¼‰
        - name_mapping: {short_name: original_name} æ˜ å°„å­—å…¸
        - short_tool_schemas: ç¼©çŸ­åç§°åçš„ tool schemas {short_name: tool_meta_with_short_name}
    """
    tools = []
    name_mapping = {}
    short_tool_schemas = {}  # short_name -> tool_meta (with short name in schema)

    for original_name, tool_meta in tool_schemas.items():
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šfunction_schema æˆ– tool_schema
        tool_schema = tool_meta.get("function_schema") or tool_meta.get("tool_schema", {})

        if tool_schema:
            short_name = shorten_tool_name(original_name)
            tool_schema_copy = copy.deepcopy(tool_schema)
            tool_meta_copy = copy.deepcopy(tool_meta)

            # ä¿®æ”¹ schema ä¸­çš„åç§°
            if "function" in tool_schema_copy:
                tool_schema_copy["function"]["name"] = short_name

            # ä¿®æ”¹ tool_meta ä¸­çš„åç§°
            tool_meta_copy["function_schema"] = tool_schema_copy

            tools.append(tool_schema_copy)
            name_mapping[short_name] = original_name
            short_tool_schemas[short_name] = tool_meta_copy

    return tools, name_mapping, short_tool_schemas


def build_hint_for_turn(turn_data: Dict, turn_type: str) -> str:
    """
    æ ¹æ® turn_type æ„å»º hints

    Args:
        turn_data: turn æ•°æ®
        turn_type: turn ç±»å‹

    Returns:
        æ ¼å¼åŒ–çš„ hint å­—ç¬¦ä¸²
    """
    if turn_type == 'normal':
        return build_normal_hint(turn_data)
    elif turn_type == 'merged':
        return build_merged_hint(turn_data)
    elif turn_type == 'insert_short':
        return build_insert_short_hint(turn_data)
    elif turn_type == 'insert_long':
        return build_insert_long_hint(turn_data)
    elif turn_type == 'insert_mixed':
        return build_insert_mixed_hint(turn_data)
    elif turn_type == 'merged_with_insert':
        return build_merged_with_insert_hint(turn_data)
    elif turn_type == 'empty':
        return build_empty_hint(turn_data)
    else:
        return build_normal_hint(turn_data)


def build_normal_hint(turn_data: Dict) -> str:
    """æ„å»º normal turn çš„ hint"""
    tool_calls = turn_data.get('tool_calls', [])

    if len(tool_calls) == 1:
        hint_lines = [f"[Hint]: You should call the function: {tool_calls[0]['function']}"]
    else:
        hint_lines = ["[Hint]: You should call the following functions:"]
        for call in tool_calls:
            hint_lines.append(f"- {call['function']}")

    hint_lines.append("")
    hint_lines.append("Remember: Do not explicitly mention these hints in your response. Use them to guide your reasoning and tool selection.")

    return "\n".join(hint_lines)


def build_merged_hint(turn_data: Dict) -> str:
    """æ„å»º merged turn çš„ hint"""
    tool_calls = turn_data.get('tool_calls', [])

    hint_lines = ["[Hint]: This turn has multiple independent intents:"]
    for call in tool_calls:
        hint_lines.append(f"- {call['function']}")

    hint_lines.extend([
        "",
        "These are separate user goals that should both be addressed.",
        "Generate reasoning that identifies each explicit intent and explains how to accomplish them."
    ])

    return "\n".join(hint_lines)


def build_insert_short_hint(turn_data: Dict) -> str:
    """æ„å»º insert_short turn çš„ hint"""
    tool_calls = turn_data.get('tool_calls', [])
    turn_operations = turn_data.get('turn_operations', {})

    # æå– insert ä¿¡æ¯
    insert_info_list = turn_operations.get('insert_info', [])
    nested_funcs = set()
    dependencies = []

    for insert_info in insert_info_list:
        nested_func_name = insert_info.get('nested_func_name')
        source_func_name = insert_info.get('source_func_name')
        insert_type = insert_info.get('insert_type')

        if nested_func_name and insert_type == 'short_dependency':
            nested_funcs.add(nested_func_name)
            if source_func_name:
                dependencies.append(f"{source_func_name} â†’ {nested_func_name}")

    # åŒºåˆ† primary å’Œ helper functions
    primary_funcs = [call['function'] for call in tool_calls if call['function'] not in nested_funcs]
    helper_funcs = [call['function'] for call in tool_calls if call['function'] in nested_funcs]

    hint_lines = ["[Hint]: Data flow for this turn:"]

    if primary_funcs:
        if len(primary_funcs) == 1:
            hint_lines.append(f"- Primary function: {primary_funcs[0]}")
        else:
            hint_lines.append(f"- Primary functions: {', '.join(primary_funcs)}")

    if helper_funcs:
        if len(helper_funcs) == 1:
            hint_lines.append(f"- Helper function: {helper_funcs[0]} (implicit, provides data for primary function)")
        else:
            hint_lines.append(f"- Helper functions: {', '.join(helper_funcs)} (implicit)")

    if dependencies:
        hint_lines.append("- Data flow: " + ", ".join(dependencies))

    hint_lines.extend([
        "",
        "Note: The helper function is implicit in the user's goal and should be incorporated naturally.",
        "Focus on the primary function's purpose, but use the helper to complete the task."
    ])

    return "\n".join(hint_lines)


def build_insert_long_hint(turn_data: Dict) -> str:
    """æ„å»º insert_long turn çš„ hint"""
    tool_calls = turn_data.get('tool_calls', [])
    turn_operations = turn_data.get('turn_operations', {})
    turn_idx = turn_data.get('turn_idx', 0)

    # æå– long-dependency ä¿¡æ¯
    insert_info_list = turn_operations.get('insert_info', [])
    long_dependencies = []

    for insert_info in insert_info_list:
        nested_func_name = insert_info.get('nested_func_name')
        source_func_name = insert_info.get('source_func_name')
        source_turn_idx = insert_info.get('source_turn_idx')
        insert_type = insert_info.get('insert_type')

        if nested_func_name and insert_type == 'long_dependency':
            if source_func_name and source_turn_idx is not None:
                long_dependencies.append({
                    'func': nested_func_name,
                    'source_turn': source_turn_idx,
                    'source_func': source_func_name
                })

    hint_lines = ["[Hint]: This turn requires data from previous conversation:"]

    for dep in long_dependencies:
        hint_lines.append(f"- Function: {dep['func']}")
        hint_lines.append(f"  Depends on: output from Turn {dep['source_turn']} ({dep['source_func']})")

    if long_dependencies:
        hint_lines.extend([
            "",
            "Important:",
            "- Use pronouns to reference previous results (e.g., 'that booking', 'those coordinates')",
            "- Do NOT repeat specific values from history",
            "- Let the data flow naturally from the conversation context"
        ])

    return "\n".join(hint_lines)


def build_insert_mixed_hint(turn_data: Dict) -> str:
    """æ„å»º insert_mixed turn çš„ hint"""
    tool_calls = turn_data.get('tool_calls', [])
    turn_operations = turn_data.get('turn_operations', {})
    turn_idx = turn_data.get('turn_idx', 0)

    # æå– insert ä¿¡æ¯
    insert_info_list = turn_operations.get('insert_info', [])
    short_deps = []
    long_deps = []

    for insert_info in insert_info_list:
        nested_func_name = insert_info.get('nested_func_name')
        source_func_name = insert_info.get('source_func_name')
        source_turn_idx = insert_info.get('source_turn_idx')
        insert_type = insert_info.get('insert_type')

        if nested_func_name:
            if insert_type == 'short_dependency' and source_func_name:
                short_deps.append(f"{source_func_name} â†’ {nested_func_name}")
            elif insert_type == 'long_dependency' and source_turn_idx is not None:
                long_deps.append({
                    'func': nested_func_name,
                    'source_turn': source_turn_idx,
                    'source_func': source_func_name
                })

    hint_lines = ["[Hint]: Mixed dependency scenario:"]
    hint_lines.append("- Functions involved: " + ", ".join([call['function'] for call in tool_calls]))

    if short_deps:
        hint_lines.append(f"- Short-dependency data flow: {', '.join(short_deps)}")

    if long_deps:
        hint_lines.append("- Long-dependency references:")
        for dep in long_deps:
            hint_lines.append(f"  * {dep['func']} depends on Turn {dep['source_turn']} ({dep['source_func']})")

    hint_lines.extend([
        "",
        "Guidance:",
        "- Use pronouns for long-dependency references",
        "- Incorporate short-dependency helpers naturally",
        "- Execute in logical order based on data dependencies"
    ])

    return "\n".join(hint_lines)


def build_merged_with_insert_hint(turn_data: Dict) -> str:
    """æ„å»º merged_with_insert turn çš„ hint"""
    tool_calls = turn_data.get('tool_calls', [])
    turn_operations = turn_data.get('turn_operations', {})
    turn_idx = turn_data.get('turn_idx', 0)

    # æå– merged å’Œ insert ä¿¡æ¯
    merge_info = turn_operations.get('merge_info', {})
    merged_names = merge_info.get('merged_names', [])

    insert_info_list = turn_operations.get('insert_info', [])
    nested_funcs = set()
    short_deps = []
    long_deps = []

    for insert_info in insert_info_list:
        nested_func_name = insert_info.get('nested_func_name')
        source_func_name = insert_info.get('source_func_name')
        source_turn_idx = insert_info.get('source_turn_idx')
        insert_type = insert_info.get('insert_type')

        if nested_func_name:
            nested_funcs.add(nested_func_name)
            if insert_type == 'short_dependency' and source_func_name:
                short_deps.append(f"{source_func_name} â†’ {nested_func_name}")
            elif insert_type == 'long_dependency' and source_turn_idx is not None:
                long_deps.append({
                    'func': nested_func_name,
                    'source_turn': source_turn_idx,
                    'source_func': source_func_name
                })

    hint_lines = ["[Hint]: Complex multi-intent scenario:"]

    # æ˜¾å¼æ„å›¾
    if merged_names:
        hint_lines.append("- Explicit intents: " + ", ".join(merged_names))

    # Helper functions
    helpers = [func for func in nested_funcs if func not in merged_names]
    if helpers:
        hint_lines.append("- Helper functions: " + ", ".join(helpers) + " (implicit)")

    # ä¾èµ–å…³ç³»
    if short_deps:
        hint_lines.append(f"- Short-dependency flow: {', '.join(short_deps)}")

    if long_deps:
        hint_lines.append("- Long-dependency references:")
        for dep in long_deps:
            hint_lines.append(f"  * {dep['func']} from Turn {dep['source_turn']}")

    hint_lines.extend([
        "",
        "Guidance:",
        "- Address all explicit intents clearly",
        "- Use pronouns for long-dependency references",
        "- Naturally incorporate helpers without explicitly mentioning them"
    ])

    return "\n".join(hint_lines)


def build_empty_hint(turn_data: Dict) -> str:
    """
    æ„å»º empty turn çš„ hint

    æ ¹æ® miss_type æä¾›ä¸åŒçš„å¼•å¯¼ï¼š
    - miss_func: ç¼ºå°‘æ‰€éœ€çš„å‡½æ•°/èƒ½åŠ›
    - miss_params: ç¼ºå°‘å¿…éœ€çš„å‚æ•°ä¿¡æ¯
    """
    miss_type = turn_data.get('miss_type', 'unknown')
    reason = turn_data.get('reason', '')

    if miss_type == 'miss_func':
        hint_lines = [
            "[Hint]: This query cannot be fulfilled.",
            "Reason: Required function is not available.",
            "",
            "Generate a polite response explaining:",
            "- That you lack the capability to perform this action",
            "- What specific function or feature is missing",
            "- Possible alternatives or how the user might proceed differently"
        ]
    elif miss_type == 'miss_params':
        hint_lines = [
            "[Hint]: This query cannot be fulfilled.",
            "Reason: Required parameters are missing or unclear.",
            "",
            "Generate a polite response explaining:",
            "- What specific information is needed to proceed",
            "- How the user can provide the missing details",
            "- Why these parameters are necessary for the request"
        ]
    else:
        # é€šç”¨ hintï¼ˆå¦‚æœ miss_type æœªçŸ¥æˆ–ä¸å­˜åœ¨ï¼‰
        hint_lines = [
            "[Hint]: This query cannot be fulfilled.",
            "",
            "Generate a polite response explaining:",
            "- Why you cannot fulfill the request",
            "- What information or capability is missing",
            "- How the user might rephrase or provide more information"
        ]

    return "\n".join(hint_lines)



async def distill_path(
    path_data: Dict,
    tool_schemas: Dict[str, Dict[str, Any]]
) -> Dict:
    """
    è’¸é¦å•ä¸ª path çš„æ‰€æœ‰ turns

    å®ç° multi-turn multi-step æµç¨‹ï¼š
    - æ¯ä¸ª turn å¯èƒ½åŒ…å«å¤šä¸ª steps
    - æ¨¡å‹å†³å®šä½•æ—¶å®Œæˆå½“å‰ turnï¼ˆä¸è¿”å› tool_callsï¼‰
    - hints åªä½œä¸ºå¼•å¯¼ï¼Œä¸å¼ºåˆ¶æ‰§è¡Œ

    Args:
        path_data: åŒ…å« path_info å’Œ turns_data çš„å­—å…¸
        tool_schemas: æ‰€æœ‰å·¥å…·çš„ schema

    Returns:
        è’¸é¦åçš„æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„å¯¹è¯å†å²
    """
    path_info = path_data['path_info']
    turns_data = path_data['turns_data']

    # æ„å»º tools for API
    tools, name_mapping, short_tool_schemas = build_tools_for_api(tool_schemas)

    # åˆå§‹åŒ–å¯¹è¯å†å²
    conversation_history = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]

    distilled_turns = []
    total_token_usage = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0
    }

    # é€è½®æ‰§è¡Œ
    for turn_idx, turn_data in enumerate(turns_data):
        turn_type = turn_data['turn_type']
        print(f"    Turn {turn_idx}, type: {turn_type}")

        # æ„å»ºè¯¥ turn çš„ prompt
        user_query = turn_data.get('user_query', '')
        hint = build_hint_for_turn(turn_data, turn_type)
        turn_prompt_with_hint = f"{user_query}\n\n{hint}"

        # ğŸ”¥ é‡è¦ï¼šconversation_history åªä¿å­˜ä¸å¸¦ hint çš„ user_query
        conversation_history.append({
            "role": "user",
            "content": user_query
        })

        # ğŸ”¥ Multi-step å¾ªç¯ï¼ˆå‚è€ƒ process_single_record_v1ï¼‰
        turn_completed = False
        max_steps_per_turn = 10
        turn_steps = []
        turn_generated_calls = []

        for step_num in range(1, max_steps_per_turn + 1):
            try:
                # ğŸ”¥ æ„å»ºåŒ…å« hint çš„ä¸´æ—¶ messages ç”¨äº API è°ƒç”¨
                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å¸¦ hint çš„ prompt
                # åç»­æ­¥éª¤ï¼šä½¿ç”¨ conversation_historyï¼ˆä¸å« hintï¼‰
                if step_num == 1:
                    # ç¬¬ä¸€æ­¥ï¼šç”¨ hint å¼•å¯¼
                    api_messages = conversation_history[:-1] + [{
                        "role": "user",
                        "content": turn_prompt_with_hint
                    }]
                else:
                    # åç»­æ­¥éª¤ï¼šç”¨å¯¹è¯å†å²ï¼ˆä¸å« hintï¼‰
                    api_messages = conversation_history

                # è°ƒç”¨æ•™å¸ˆæ¨¡å‹
                completion = await async_client.chat.completions.create(
                    model=TEACHER_MODEL,
                    messages=api_messages,
                    tools=tools,
                    temperature=0.7,
                    max_completion_tokens=2048
                )

                message = completion.choices[0].message

                # ç´¯è®¡ token ä½¿ç”¨
                total_token_usage['prompt_tokens'] += completion.usage.prompt_tokens
                total_token_usage['completion_tokens'] += completion.usage.completion_tokens
                total_token_usage['total_tokens'] += completion.usage.total_tokens

                # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
                if not message.tool_calls:
                    # æ²¡æœ‰ tool calls â†’ turn å®Œæˆï¼Œè¿™å°±æ˜¯æ€»ç»“
                    print(f"      Step {step_num}: Turn completed (summary)")
                    conversation_history.append({
                        "role": "assistant",
                        "content": message.content or ""
                    })

                    turn_steps.append({
                        "step_num": step_num,
                        "type": "summary",
                        "content": message.content or ""
                    })

                    turn_completed = True
                    break

                # æœ‰ tool calls â†’ æ‰§è¡Œå‡½æ•°
                print(f"      Step {step_num}: {len(message.tool_calls)} tool calls")

                # æ˜ å°„å›åŸå§‹å‡½æ•°å
                original_tool_calls = []
                for api_call in message.tool_calls:
                    short_name = api_call.function.name
                    original_name = name_mapping.get(short_name, short_name)
                    try:
                        params = json.loads(api_call.function.arguments)
                    except json.JSONDecodeError:
                        params = {}
                    original_tool_calls.append({
                        "function": original_name,
                        "parameters": params
                    })
                    turn_generated_calls.append(original_name)

                # æ·»åŠ  assistant messageï¼ˆå¸¦ tool_callsï¼‰
                conversation_history.append({
                    "role": "assistant",
                    "content": message.content or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in message.tool_calls
                    ]
                })

                # æ‰§è¡Œå‡½æ•°å¹¶æ·»åŠ  tool messages
                step_outputs = []
                for i, tc in enumerate(message.tool_calls):
                    short_func_name = tc.function.name
                    original_func_name = name_mapping.get(short_func_name, short_func_name)

                    try:
                        params = json.loads(tc.function.arguments)
                    except json.JSONDecodeError:
                        params = {}

                    try:
                        # å®é™…æ‰§è¡Œå‡½æ•°
                        output_result = await execute_function_call(
                            original_func_name,
                            params
                        )

                        # æå–å®é™…çš„ outputï¼ˆç§»é™¤ token_usageï¼‰
                        if output_result:
                            if "token_usage" in output_result:
                                output = {k: v for k, v in output_result.items() if k != "token_usage"}
                            else:
                                output = output_result
                        else:
                            output = {"error": "Function execution returned None"}

                        step_outputs.append({
                            "function": original_func_name,
                            "output": output
                        })

                        # æ·»åŠ  tool message
                        conversation_history.append({
                            "role": "tool",
                            "tool_call_id": tc.id,
                            "content": json.dumps(output, ensure_ascii=False)
                        })

                    except Exception as e:
                        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                        raise RuntimeError(
                            f"Error executing function {original_func_name} with parameters {params} at turn {turn_idx}, step {step_num}: {e}"
                        ) from e

                turn_steps.append({
                    "step_num": step_num,
                    "type": "tool_calls",
                    "reasoning": message.content or "",
                    "tool_calls": original_tool_calls,
                    "outputs": step_outputs
                })

            except Exception as e:
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                raise RuntimeError(
                    f"Error at turn {turn_idx}, step {step_num}: {e}"
                ) from e

        if not turn_completed:
            print(f"      Warning: Turn {turn_idx} reached max steps ({max_steps_per_turn})")

        # è®°å½•è¯¥ turn çš„ç»“æœ
        turn_result = {
            "turn_idx": turn_idx,
            "turn_type": turn_type,
            "user_query": user_query,
            "hint": hint,
            "steps": turn_steps,
            "total_steps": len(turn_steps),
            "ground_truth_tool_calls": turn_data.get('tool_calls', []),
            "generated_tool_calls": turn_generated_calls
        }

        # Empty turn çš„é¢å¤–ä¿¡æ¯
        if turn_type == 'empty':
            turn_result["miss_type"] = turn_data.get('miss_type', 'unknown')
            turn_result["ground_truth_response"] = turn_data.get('response', '')
            turn_result["reason"] = turn_data.get('reason', '')

        distilled_turns.append(turn_result)

    # æ„å»º tools åˆ—è¡¨ï¼šä½¿ç”¨ç¼©çŸ­åç§°çš„ schemas
    tools_list = [
        {"function_schema": tool_meta.get("function_schema", {})}
        for tool_meta in short_tool_schemas.values()
        if "function_schema" in tool_meta
    ]

    return {
        "path_info": path_info,
        "conversation_history": conversation_history,
        "distilled_turns": distilled_turns,
        "token_usage": total_token_usage,
        "statistics": compute_statistics(distilled_turns),
        "tools": tools_list,
        "tool_name_mapping": name_mapping
    }


def compute_statistics(distilled_turns: List[Dict]) -> Dict:
    """è®¡ç®—è’¸é¦ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯"""
    total_turns = len(distilled_turns)
    total_steps = sum(turn['total_steps'] for turn in distilled_turns)
    total_tool_calls = sum(len(turn['generated_tool_calls']) for turn in distilled_turns)

    # è®¡ç®—å‡½æ•°åŒ¹é…ç‡
    function_matches = 0
    total_functions = 0

    for turn in distilled_turns:
        gt_funcs = set(call['function'] for call in turn['ground_truth_tool_calls'])
        gen_funcs = set(turn['generated_tool_calls'])

        function_matches += len(gt_funcs & gen_funcs)
        total_functions += len(gt_funcs)

    function_match_rate = function_matches / total_functions if total_functions > 0 else 0.0

    return {
        "num_turns": total_turns,
        "total_steps": total_steps,
        "avg_steps_per_turn": total_steps / total_turns if total_turns > 0 else 0,
        "num_tool_calls": total_tool_calls,
        "function_match_rate": function_match_rate
    }


async def run_distillation_v2(
    max_paths: Optional[int] = None,
    batch_size: int = 5,
    resume: bool = False,
    early_stop_batches: int = 3
) -> None:
    """
    è¿è¡Œæ­£å‘è’¸é¦ V2

    Args:
        max_paths: æœ€å¤šå¤„ç†çš„ path æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²æˆåŠŸå¤„ç†çš„ pathï¼‰
        early_stop_batches: è¿ç»­å¤šå°‘ä¸ª batch å…¨éƒ¨å¤±è´¥ååœæ­¢ï¼ˆ0 è¡¨ç¤ºä¸å¯ç”¨æ—©åœï¼‰
    """
    print(f"Loading FSP V2 data from {FSP_V2_PATH}...")

    paths = []
    with open(FSP_V2_PATH, "r", encoding="utf-8") as f:
        for line in f:
            paths.append(json.loads(line))
            if max_paths and len(paths) >= max_paths:
                break

    print(f"Loaded {len(paths)} paths.")

    # ğŸ”¥ åŠ è½½æ‰€æœ‰ tool schemas
    print(f"Loading tool schemas from {TOOL_SCHEMA_PATH}...")
    all_tool_schemas = load_all_tool_schemas()
    print(f"Loaded {len(all_tool_schemas)} tool schemas.")

    print(f"Output -> {DISTILL_V2_OUTPUT}\n")

    os.makedirs(os.path.dirname(DISTILL_V2_OUTPUT), exist_ok=True)

    # æ–­ç‚¹ç»­ä¼ ï¼šè¯»å–å·²æˆåŠŸå¤„ç†çš„ path
    successfully_processed_paths = set()

    if resume and os.path.exists(DISTILL_V2_OUTPUT):
        print(f"\nğŸ”„ Resume mode enabled, reading existing results from {DISTILL_V2_OUTPUT}...")
        try:
            with open(DISTILL_V2_OUTPUT, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        result = json.loads(line)
                        path_info = result.get('path_info', {})
                        node_idx = path_info.get('node_idx')
                        path_idx = path_info.get('path_idx')

                        # åªè®°å½•æˆåŠŸå¤„ç†çš„ï¼ˆæ²¡æœ‰ error å­—æ®µï¼‰
                        if node_idx is not None and path_idx is not None and 'error' not in result:
                            successfully_processed_paths.add((node_idx, path_idx))
                    except json.JSONDecodeError:
                        continue

            print(f"âœ… Found {len(successfully_processed_paths)} successfully processed paths.")

            # è¿‡æ»¤æ‰å·²å¤„ç†çš„ paths
            original_count = len(paths)
            paths = [
                p for p in paths
                if (p.get('path_info', {}).get('node_idx'),
                    p.get('path_info', {}).get('path_idx')) not in successfully_processed_paths
            ]

            skipped_count = original_count - len(paths)
            print(f"ğŸ“‹ Skipping {skipped_count} already processed paths.")
            print(f"ğŸ“‹ Remaining paths to process: {len(paths)}\n")

            if len(paths) == 0:
                print("\nâœ… All paths already processed! Nothing to do.")
                return

        except Exception as e:
            print(f"âš ï¸  Warning: Failed to read existing results: {e}")
            print("   Continuing without resume...")
            successfully_processed_paths.clear()

    # æ–‡ä»¶å†™å…¥æ¨¡å¼ï¼šresume ä½¿ï¿½ï¿½è¿½åŠ æ¨¡å¼ï¼Œå¦åˆ™ä½¿ç”¨è¦†ç›–æ¨¡å¼
    file_mode = "a" if resume else "w"
    if file_mode == "a":
        print(f"ğŸ“ Appending to existing file: {DISTILL_V2_OUTPUT}\n")

    total_tokens = 0
    total_processed = 0
    total_errors = 0
    total_function_matches = 0
    total_functions = 0
    consecutive_failed_batches = 0

    with open(DISTILL_V2_OUTPUT, file_mode, encoding="utf-8") as f:
        num_batches = (len(paths) + batch_size - 1) // batch_size

        for batch_idx, start in enumerate(tqdm(range(0, len(paths), batch_size),
                                               total=num_batches,
                                               desc="Processing paths",
                                               unit="batch"), start=1):
            batch = paths[start: start + batch_size]
            print(f"\n[Batch {batch_idx}/{num_batches}] Processing paths {start + 1}-{start + len(batch)}...")

            batch_errors = 0
            batch_tokens = 0

            # å‡†å¤‡ batch tasksï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
            tasks = []
            task_path_info = []  # è®°å½•æ¯ä¸ª task å¯¹åº”çš„ path_infoï¼Œç”¨äºé”™è¯¯æŠ¥å‘Š

            for path_data in batch:
                path_info = path_data.get('path_info', {})

                # æå– tool_schemas
                tool_schemas = extract_tool_schemas_for_path(path_data, all_tool_schemas)

                if not tool_schemas:
                    print(f"  Path {path_info.get('node_idx', '?')}-{path_info.get('path_idx', '?')}: Warning: No tool schemas found")
                    batch_errors += 1
                    total_errors += 1
                    continue

                # åˆ›å»º task
                tasks.append(distill_path(path_data, tool_schemas))
                task_path_info.append(path_info)

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ tasks
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœ
                for result, path_info in zip(results, task_path_info):
                    if isinstance(result, Exception):
                        print(f"  Path {path_info.get('node_idx', '?')}-{path_info.get('path_idx', '?')}: [ERROR] {result}")
                        import traceback
                        traceback.print_exc()
                        batch_errors += 1
                        total_errors += 1
                        continue

                    # å†™å…¥æˆåŠŸç»“æœ
                    f.write(json.dumps(result, ensure_ascii=False) + "\n")
                    f.flush()

                    # ç»Ÿè®¡
                    total_processed += 1
                    token_usage = result.get('token_usage', {})
                    batch_tokens += token_usage.get('total_tokens', 0)

                    # ç»Ÿè®¡å‡½æ•°åŒ¹é…ç‡
                    distilled_turns = result.get('distilled_turns', [])
                    for turn in distilled_turns:
                        gt_funcs = set(call['function'] for call in turn.get('ground_truth_tool_calls', []))
                        gen_funcs = set(turn.get('generated_tool_calls', []))
                        total_function_matches += len(gt_funcs & gen_funcs)
                        total_functions += len(gt_funcs)

            total_tokens += batch_tokens

            # è®¡ç®—å½“å‰ batch çš„ç»Ÿè®¡
            batch_success = len(batch) - batch_errors
            print(f"[Batch {batch_idx}] Success: {batch_success}/{len(batch)}, "
                  f"batch_tokens={batch_tokens}, overall_tokens={total_tokens}")

            # æ—©åœæ£€æŸ¥ï¼šå¦‚æœå½“å‰ batch å…¨éƒ¨å¤±è´¥
            if batch_errors == len(batch):
                consecutive_failed_batches += 1
                print(
                    f"[Batch {batch_idx}] âŒ ALL {len(batch)} paths FAILED! "
                    f"(consecutive failed batches: {consecutive_failed_batches}/{early_stop_batches})"
                )

                # å¦‚æœè¿ç»­å¤±è´¥çš„ batch æ•°é‡è¾¾åˆ°é˜ˆå€¼ï¼Œåœæ­¢å¤„ç†
                if early_stop_batches > 0 and consecutive_failed_batches >= early_stop_batches:
                    print("\n" + "=" * 80)
                    print("ğŸ›‘ EARLY STOPPING TRIGGERED")
                    print("=" * 80)
                    print(f"Consecutive failed batches: {consecutive_failed_batches}")
                    print(f"Stopping to prevent further failures...")
                    print("=" * 80)
                    break
            else:
                # åªè¦æœ‰ä¸€ä¸ªæˆåŠŸï¼Œå°±é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                consecutive_failed_batches = 0

    # è®¡ç®—æ•´ä½“å‡½æ•°åŒ¹é…ç‡
    overall_match_rate = total_function_matches / total_functions if total_functions > 0 else 0.0

    print("\n" + "=" * 80)
    print("DISTILLATION V2 SUMMARY")
    print("=" * 80)
    print(f"Total paths: {len(paths)}")
    print(f"Processed: {total_processed}")
    print(f"Failed: {total_errors}")
    print(f"Success rate: {total_processed / len(paths) * 100:.1f}%" if len(paths) > 0 else "N/A")
    print(f"Total tokens used: {total_tokens}")
    print(f"Function match rate: {overall_match_rate:.2%} ({total_function_matches}/{total_functions})")
    print("=" * 80)
    print(f"\nResults saved to: {DISTILL_V2_OUTPUT}")


def main() -> None:
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="Run positive distillation V2")
    parser.add_argument('--max-paths', type=int, default=None,
                        help='Maximum number of paths to process (for testing)')
    parser.add_argument('--batch-size', type=int, default=1,
                        help='Batch size for processing')
    parser.add_argument('--resume', action='store_true',
                        help='Resume from previous run (skip already processed paths)')
    parser.add_argument('--early-stop', type=int, default=3,
                        help='Stop after N consecutive batches with all failures (0 to disable, default: 3)')
    parser.add_argument('--test', action='store_true',
                        help='Test mode: process only 2 paths')

    args = parser.parse_args()

    if args.test:
        print("ğŸ§ª Running in TEST mode (2 paths only)...")
        args.max_paths = 2

    asyncio.run(run_distillation_v2(
        max_paths=args.max_paths,
        batch_size=args.batch_size,
        resume=args.resume,
        early_stop_batches=args.early_stop
    ))


if __name__ == "__main__":
    main()
