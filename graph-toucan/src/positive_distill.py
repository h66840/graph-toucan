"""
æ­£å‘è’¸é¦è„šæœ¬ï¼šéªŒè¯ backward_to_query ç”Ÿæˆçš„ user query è´¨é‡

æµç¨‹ï¼š
1. è¯»å– backward_to_query ç”Ÿæˆçš„ç»“æœï¼ˆfinal_query + tool_schema + fc_referenceï¼‰
2. å°† final_query å’Œ tool_schema ç»™å¤§æ¨¡å‹ï¼Œè®©å…¶æ­£å‘ rolloutï¼ˆå¤šæ­¥æ‰§è¡Œï¼‰
3. å¯¹æ¯”ç”Ÿæˆçš„ fc_reference å’ŒåŸå§‹çš„ fc_referenceï¼Œè®¡ç®—å‡†ç¡®ç‡

è¯„ä¼°æŒ‡æ ‡ï¼š
- å‡½æ•°è°ƒç”¨å‡†ç¡®ç‡ï¼ˆå‡½æ•°ååŒ¹é…ï¼‰
- å‚æ•°å‡†ç¡®ç‡ï¼ˆå‚æ•°é”®å’Œå€¼åŒ¹é…ï¼‰
- Step é¡ºåºå‡†ç¡®ç‡
"""

import asyncio
import json
import os
import sys
import yaml
import hashlib
import copy
from typing import Any, Dict, List, Optional, Tuple
from tqdm import tqdm
from openai import AsyncOpenAI

# å¯¼å…¥ backward_to_query ä¸­çš„å‡½æ•°
sys.path.insert(0, os.path.dirname(__file__))
from backward_to_query import execute_function_call


# è·¯å¾„é…ç½®
ROOT_DIR = "/data/lhy/datasets/graph-Toucan"
GRAPH_DIR = os.path.join(ROOT_DIR, "graph")
FSP_DIR = "/data/lhy/datasets/graph-Toucan/fsp_path"
DISTILL_DIR = "/data/lhy/datasets/graph-Toucan/distill"
BACKWARD_QUERIES_PATH = os.path.join(FSP_DIR, "fsp_v1.json")
OUTPUT_DISTILL_RESULTS_PATH = os.path.join(DISTILL_DIR, "distill_v1.jsonl")
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")


def load_config(config_path: str = CONFIG_PATH) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


# åŠ è½½é…ç½®
config = load_config()

# åˆå§‹åŒ– AsyncOpenAI å®¢æˆ·ç«¯
# æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š
# 1. é€šè¿‡ç¯å¢ƒå˜é‡ï¼šapi_key_env: "DASHSCOPE_API_KEY"
# 2. ç›´æ¥é…ç½®ï¼šapi_key: "EMPTY"
api_key_env = config["api"].get("api_key_env")
if api_key_env:
    # å¦‚æœé…ç½®äº† api_key_envï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
    api_key = os.getenv(api_key_env, "EMPTY")
else:
    # å¦åˆ™ç›´æ¥ä»é…ç½®è¯»å–
    api_key = config["api"].get("api_key", "EMPTY")
base_url = config["api"]["base_url"]

async_client = AsyncOpenAI(
    api_key=api_key,
    base_url=base_url,
)

# æ¨¡å‹é…ç½®
DEFAULT_MODEL = config["model"]["default"]


def build_tool_schema_prompt(nodes_tool_schema: Dict[str, Dict[str, Any]]) -> str:
    """
    æ„å»ºå·¥å…· schema çš„ prompt æè¿°

    Args:
        nodes_tool_schema: å·¥å…· schema å­—å…¸

    Returns:
        æ ¼å¼åŒ–çš„å·¥å…·æ–‡æ¡£å­—ç¬¦ä¸²
    """
    tool_docs = []
    for tool_name, tool_meta in nodes_tool_schema.items():
        tool_schema = tool_meta.get("tool_schema", {}).get("function", {})
        description = tool_schema.get("description", "")
        params = tool_schema.get("parameters", {})

        # æ„å»ºå‚æ•°æ–‡æ¡£
        param_lines = []
        if params and isinstance(params, dict):
            properties = params.get("properties", {})
            required = set(params.get("required", []))

            for param_name, param_info in properties.items():
                if not isinstance(param_info, dict):
                    continue
                param_type = param_info.get("type", "unknown")
                param_desc = param_info.get("description", "")
                req_flag = "required" if param_name in required else "optional"
                param_lines.append(f"    - {param_name} ({param_type}, {req_flag}): {param_desc}")

        param_block = "\n".join(param_lines) if param_lines else "    (no parameters)"

        tool_doc = f"""- {tool_name}:
  Description: {description}
  Parameters:
{param_block}
"""
        tool_docs.append(tool_doc)

    return "\n".join(tool_docs)


def parse_tool_calls_from_content(content: str) -> List[Dict[str, Any]]:
    """
    ä» LLM è¾“å‡ºä¸­è§£æ tool calls

    æ ¼å¼:
    tool_call1: function_name with parameters: {...}
    tool_call2: function_name with parameters: {...}

    Args:
        content: LLM çš„è¾“å‡ºå†…å®¹

    Returns:
        tool_calls åˆ—è¡¨
    """
    tool_calls = []
    lines = content.split('\n')

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # è§£æ tool_call éƒ¨åˆ†
        if line.startswith("tool_call"):
            try:
                # æå–å‡½æ•°åå’Œå‚æ•°
                if "with parameters:" in line:
                    parts = line.split("with parameters:")
                    func_part = parts[0].strip()
                    params_part = parts[1].strip()

                    # æå–å‡½æ•°å (tool_call1: function_name)
                    func_name = func_part.split(":")[-1].strip()

                    # è§£æ JSON å‚æ•°
                    try:
                        params = json.loads(params_part)
                    except json.JSONDecodeError:
                        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨ç©ºå‚æ•°
                        params = {}

                    tool_calls.append({
                        "function": func_name,
                        "parameters": params
                    })
            except Exception as e:
                print(f"Warning: Failed to parse tool_call line: {line}, error: {e}")

    return tool_calls


def shorten_tool_name(name: str, max_length: int = 64) -> str:
    """
    ç¼©çŸ­å·¥å…·åç§°åˆ°æŒ‡å®šé•¿åº¦

    Args:
        name: åŸå§‹å·¥å…·åç§°
        max_length: æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤64ï¼‰

    Returns:
        ç¼©çŸ­åçš„å·¥å…·åç§°
    """
    if len(name) <= max_length:
        return name

    # ç”Ÿæˆ5ä½å“ˆå¸Œåç¼€ä¿è¯å”¯ä¸€æ€§
    hash_suffix = hashlib.md5(name.encode()).hexdigest()[:5]

    # ä¿ç•™å‰ max_length-6 ä¸ªå­—ç¬¦ + "_" + å“ˆå¸Œ
    max_prefix = max_length - 6
    return f"{name[:max_prefix]}_{hash_suffix}"


def build_tools_for_api(
    tool_schemas: Dict[str, Dict[str, Any]]
) -> Tuple[List[Dict[str, Any]], Dict[str, str], Dict[str, Dict[str, Any]]]:
    """
    å°† tool schemas è½¬æ¢ä¸º OpenAI API æ ¼å¼çš„ tools åˆ—è¡¨

    Args:
        tool_schemas: å·¥å…· schema å­—å…¸ {original_name: tool_meta}

    Returns:
        - tools: OpenAI API æ ¼å¼çš„ tools åˆ—è¡¨ï¼ˆåç§°å·²ç¼©çŸ­ï¼‰
        - name_mapping: {short_name: original_name} æ˜ å°„å­—å…¸
        - short_tool_schemas: ç¼©çŸ­åç§°åçš„ tool schemas {short_name: tool_meta_with_short_name}
    """
    tools = []
    name_mapping = {}  # short_name -> original_name
    short_tool_schemas = {}  # short_name -> tool_meta (with short name in schema)

    for original_name, tool_meta in tool_schemas.items():
        tool_schema = tool_meta.get("function_schema", {})
        if tool_schema:
            # ç¼©çŸ­åç§°
            short_name = shorten_tool_name(original_name)

            # æ·±æ‹·è´é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            tool_schema_copy = copy.deepcopy(tool_schema)
            tool_meta_copy = copy.deepcopy(tool_meta)

            # ä¿®æ”¹ schema ä¸­çš„åç§°
            if "function" in tool_schema_copy:
                tool_schema_copy["function"]["name"] = short_name

            # ä¿®æ”¹ tool_meta ä¸­çš„åç§°
            tool_meta_copy["function_schema"] = tool_schema_copy

            tools.append(tool_schema_copy)
            name_mapping[short_name] = original_name
            short_tool_schemas[short_name] = tool_meta_copy

    return tools, name_mapping, short_tool_schemas


async def forward_rollout_step(
    user_query: str,
    tool_schemas: Dict[str, Dict[str, Any]],
    conversation_history: List[Dict[str, str]],
    max_steps: int = 10,
    model: str = None
) -> Dict[str, Any]:
    """
    æ­£å‘ rolloutï¼šå•æ­¥æ‰§è¡Œï¼Œæ ¹æ® user query å’Œå¯¹è¯å†å²ç”Ÿæˆä¸‹ä¸€æ­¥å‡½æ•°è°ƒç”¨

    è¿™æ˜¯ä¸€ä¸ª single turn multi-step çš„è¿‡ç¨‹ï¼š
    1. LLM æ ¹æ® user query å’Œå†å²ç”Ÿæˆå½“å‰ step çš„ tool calls
    2. æ‰§è¡Œè¿™äº› tool callsï¼Œè·å– outputs
    3. å°† tool outputs åŠ å…¥ message history
    4. ç»§ç»­ä¸‹ä¸€ä¸ª stepï¼Œç›´åˆ° LLM è®¤ä¸ºä»»åŠ¡å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°

    Args:
        user_query: ç”¨æˆ·æŸ¥è¯¢
        tool_schemas: å¯ç”¨å·¥å…·çš„ schema
        conversation_history: å¯¹è¯å†å²ï¼ˆåˆå§‹ä¸ºç©ºï¼‰
        max_steps: æœ€å¤§æ­¥æ•°
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ default æ¨¡å‹ï¼‰

    Returns:
        åŒ…å«æ‰€æœ‰ steps å’Œ token ä½¿ç”¨ä¿¡æ¯çš„å­—å…¸
    """
    if model is None:
        model = DEFAULT_MODEL

    # æ„å»º OpenAI API æ ¼å¼çš„ toolsï¼ˆåŒ…å«åç§°æ˜ å°„ï¼‰
    tools, name_mapping, short_tool_schemas = build_tools_for_api(tool_schemas)

    all_steps = []
    total_token_usage = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0,
    }

    # åˆå§‹åŒ–å¯¹è¯å†å²ï¼ˆå¦‚æœä¸ºç©ºï¼‰
    if not conversation_history:
        conversation_history = [
            {
                "role": "system",
                "content": (
                    "You are a multi-step function-calling agent. "
                    "When you try to solve user's query, you should provide your thinking process and proper function call,you need give your thinking process to user,And give the proper function call "
                    "Your thinking process should explain:\n"
                    "What you understand about the current task\n"
                    "Why you are choosing to call specific tools\n"
                    "What information you expect to get from the tool calls\n"
                    "How you plan to use the results\n\n"
                    "Then provide the appropriate function calls. "
                    
                    "Your thinking process should be written as a coherent narrative paragraph (not bullet points), explaining what you understand about the current task, "
                    "Think step by step and generate function calls to accomplish the user's task. "
                    "After each step, you will receive the tool outputs. "
                    "Continue until the task is complete."
                    "always use english language"
                )
            },
            {
                "role": "user",
                "content": user_query
            }
        ]

    # å¤šæ­¥æ‰§è¡Œå¾ªç¯
    for step_num in range(1, max_steps + 1):
        try:
            # è°ƒç”¨ LLM ç”Ÿæˆå½“å‰æ­¥éª¤çš„å‡½æ•°è°ƒç”¨ï¼ˆä½¿ç”¨æ ‡å‡† function calling APIï¼‰
            completion = await async_client.chat.completions.create(
                model=model,
                messages=conversation_history,
                tools=tools,  # ä¼ é€’æ ‡å‡†çš„ tools
                stream=False,
                temperature=1,
                max_completion_tokens=1024,
            )

            message = completion.choices[0].message

            # ç´¯è®¡ token ä½¿ç”¨
            usage = completion.usage
            if usage:
                total_token_usage["prompt_tokens"] += usage.prompt_tokens
                total_token_usage["completion_tokens"] += usage.completion_tokens
                total_token_usage["total_tokens"] += usage.total_tokens

            # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
            if not message.tool_calls:
                # æ²¡æœ‰ tool callsï¼Œä»»åŠ¡å®Œæˆï¼Œè®°å½•æœ€åçš„æ€»ç»“
                final_step = {
                    "step_num": step_num,
                    "summary": message.content or "",
                    "tool_calls": [],
                    "tool_outputs": []
                }
                all_steps.append(final_step)

                # å°†æœ€åçš„ assistant æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
                conversation_history.append({
                    "role": "assistant",
                    "content": message.content or ""
                })

                print(f"  Task completed at step {step_num} (no more tool calls)")
                break

            # æå– tool_calls
            api_tool_calls = message.tool_calls

            # è®°å½•å½“å‰æ­¥éª¤
            tool_calls_for_record = []
            for tc in api_tool_calls:
                try:
                    params = json.loads(tc.function.arguments) if tc.function.arguments else {}
                except json.JSONDecodeError:
                    params = {}

                tool_calls_for_record.append({
                    "function": tc.function.name,
                    "parameters": params
                })

            current_step = {
                "step_num": step_num,
                "tool_calls": tool_calls_for_record,
                "tool_outputs": []
            }

            # æ‰§è¡Œå‡½æ•°è°ƒç”¨
            step_outputs = []
            tool_messages = []  # ç”¨äºæ„å»º tool role æ¶ˆæ¯

            for tc in api_tool_calls:
                short_func_name = tc.function.name
                tool_call_id = tc.id

                try:
                    parameters = json.loads(tc.function.arguments) if tc.function.arguments else {}
                except json.JSONDecodeError:
                    parameters = {}

                try:
                    # æ˜ å°„å›åŸå§‹åç§°æ‰§è¡Œ
                    original_func_name = name_mapping.get(short_func_name, short_func_name)
                    output_result = await execute_function_call(original_func_name, parameters)

                    # æå–å®é™…çš„ result
                    if isinstance(output_result, dict):
                        # å¦‚æœæœ‰ "result" å­—æ®µï¼Œæå–å®ƒï¼›å¦åˆ™ä½¿ç”¨æ•´ä¸ª dict
                        if "result" in output_result:
                            output = output_result["result"]
                        else:
                            # æ²¡æœ‰ "result" å­—æ®µï¼Œä½¿ç”¨æ•´ä¸ª dict ä½†ç§»é™¤ token_usage
                            output = {k: v for k, v in output_result.items() if k != "token_usage"}

                        # ç´¯åŠ  execute_function_call çš„ token ä½¿ç”¨
                        tq = output_result.get("token_usage", {})
                        total_token_usage["prompt_tokens"] += tq.get("prompt_tokens", 0)
                        total_token_usage["completion_tokens"] += tq.get("completion_tokens", 0)
                        total_token_usage["total_tokens"] += tq.get("total_tokens", 0)
                    else:
                        output = output_result

                    step_outputs.append({
                        "function": short_func_name,  # ä½¿ç”¨ç¼©çŸ­çš„åç§°
                        "output": output
                    })

                    # æ„å»º tool message
                    tool_output_str = json.dumps(output, ensure_ascii=False) if isinstance(output, (dict, list)) else str(output)
                    tool_messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": tool_output_str
                    })

                except Exception as e:
                    print(f"  Error executing {short_func_name}: {e}")
                    step_outputs.append({
                        "function": short_func_name,  # ä½¿ç”¨ç¼©çŸ­çš„åç§°
                        "error": str(e)
                    })

                    # å³ä½¿å‡ºé”™ä¹Ÿè¦æ·»åŠ  tool message
                    tool_messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                    })

            current_step["tool_outputs"] = step_outputs
            all_steps.append(current_step)

            # å°† assistant messageï¼ˆå¸¦ tool_callsï¼‰æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation_history.append({
                "role": "assistant",
                "content": message.content or "",
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": tc.type,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    }
                    for tc in api_tool_calls
                ]
            })

            # æ·»åŠ æ‰€æœ‰ tool messages
            conversation_history.extend(tool_messages)

        except Exception as e:
            print(f"  Error at step {step_num}: {e}")
            return {
                "error": f"Error at step {step_num}: {e}",
                "steps": all_steps,
                "token_usage": total_token_usage
            }

    return {
        "steps": all_steps,
        "token_usage": total_token_usage,
        "total_steps": len(all_steps),
        "conversation_history": conversation_history,
        "short_tool_schemas": short_tool_schemas,
        "tool_name_mapping": name_mapping
    }


def compare_function_calls_v1(
    ground_truth: List[Dict[str, Any]],
    generated: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    å¯¹æ¯” ground truth å’Œ generated çš„å‡½æ•°è°ƒç”¨ï¼ˆå¤šè½®å¯¹è¯ç‰ˆæœ¬ï¼ŒæŒ‰ turn æ¯”è¾ƒï¼‰

    è¿™ä¸ªç‰ˆæœ¬ä¸“é—¨ç”¨äºå¤šè½®å¯¹è¯ï¼š
    - ground_truth çš„æ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ª turn çš„ tool calls
    - generated ä¸­çš„ steps éœ€è¦æŒ‰ turn_idx åˆ†ç»„åå†æ¯”è¾ƒ
    - åªæ¯”è¾ƒåŒä¸€ä¸ª turn å†…çš„ function names

    Args:
        ground_truth: åŸå§‹çš„ fc_resultsï¼ˆæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ª turnï¼ŒåŒ…å« tool_callsï¼‰
        generated: æ­£å‘ç”Ÿæˆçš„ stepsï¼ˆæ¯ä¸ª step åŒ…å« turn_idx å’Œ tool_callsï¼‰

    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«ï¼š
        - total_gt_turns: ground truth æ€»è½®æ•°
        - total_gen_turns: generated æ€»è½®æ•°ï¼ˆä» steps ä¸­æå–çš„å”¯ä¸€ turn_idx æ•°é‡ï¼‰
        - turn_matches: åŒ¹é…çš„ turn æ•°é‡
        - turn_details: æ¯ä¸ª turn çš„è¯¦ç»†å¯¹æ¯”ä¿¡æ¯
        - overall_accuracy: turn åŒ¹é…å‡†ç¡®ç‡
        - exact_match: æ˜¯å¦æ‰€æœ‰ turn éƒ½åŒ¹é…
    """
    metrics = {
        "total_gt_turns": len(ground_truth),
        "total_gen_turns": 0,
        "turn_matches": 0,
        "turn_details": [],
    }

    # å°† generated steps æŒ‰ turn_idx åˆ†ç»„
    gen_by_turn = {}
    for step in generated:
        turn_idx = step.get("turn_idx")
        if turn_idx is not None:
            if turn_idx not in gen_by_turn:
                gen_by_turn[turn_idx] = []
            gen_by_turn[turn_idx].append(step)

    metrics["total_gen_turns"] = len(gen_by_turn)

    # é€ turn å¯¹æ¯”
    max_turns = max(len(ground_truth), len(gen_by_turn) if gen_by_turn else 0)

    for turn_idx in range(1, max_turns + 1):
        turn_detail = {
            "turn_idx": turn_idx,
            "gt_functions": [],
            "gen_functions": [],
            "match": False,
            "missing_functions": [],  # åœ¨ GT ä¸­æœ‰ä½† generated ä¸­ç¼ºå¤±
            "extra_functions": [],     # generated ä¸­æœ‰ä½† GT ä¸­æ²¡æœ‰
        }

        # æå– ground truth è¯¥ turn çš„æ‰€æœ‰å‡½æ•°å
        if turn_idx - 1 < len(ground_truth):
            gt_turn = ground_truth[turn_idx - 1]
            tool_calls = gt_turn.get("tool_calls", [])
            for tc in tool_calls:
                if isinstance(tc, dict):
                    func_name = tc.get("function", "")
                    if func_name:
                        turn_detail["gt_functions"].append(func_name)

        # æå– generated è¯¥ turn çš„æ‰€æœ‰å‡½æ•°åï¼ˆåˆå¹¶è¯¥ turn æ‰€æœ‰ steps çš„å‡½æ•°è°ƒç”¨ï¼‰
        if turn_idx in gen_by_turn:
            for step in gen_by_turn[turn_idx]:
                tool_calls = step.get("tool_calls", [])
                for tc in tool_calls:
                    if isinstance(tc, dict):
                        func_name = tc.get("function", "")
                        if func_name:
                            turn_detail["gen_functions"].append(func_name)

        # è½¬æ¢ä¸ºé›†åˆè¿›è¡Œæ¯”è¾ƒï¼ˆå»é‡ï¼‰
        gt_funcs_set = set(turn_detail["gt_functions"])
        gen_funcs_set = set(turn_detail["gen_functions"])

        # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…
        if gt_funcs_set == gen_funcs_set:
            turn_detail["match"] = True
            metrics["turn_matches"] += 1
        else:
            # æ‰¾å‡ºç¼ºå¤±å’Œå¤šä½™çš„å‡½æ•°
            turn_detail["missing_functions"] = list(gt_funcs_set - gen_funcs_set)
            turn_detail["extra_functions"] = list(gen_funcs_set - gt_funcs_set)

        metrics["turn_details"].append(turn_detail)

    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    if metrics["total_gt_turns"] > 0:
        metrics["overall_accuracy"] = metrics["turn_matches"] / metrics["total_gt_turns"]
    else:
        metrics["overall_accuracy"] = 0.0

    # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆæ‰€æœ‰ turns éƒ½åŒ¹é…ï¼‰
    metrics["exact_match"] = (
        metrics["total_gt_turns"] == metrics["total_gen_turns"] and
        metrics["turn_matches"] == metrics["total_gt_turns"]
    )

    return metrics


def compare_function_calls(
    ground_truth: List[Dict[str, Any]],
    generated: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    å¯¹æ¯” ground truth å’Œ generated çš„å‡½æ•°è°ƒç”¨ï¼ˆåªæ¯”è¾ƒå‡½æ•°åï¼‰

    Args:
        ground_truth: åŸå§‹çš„ fc_resultsï¼ˆæ¯ä¸ªå…ƒç´ åŒ…å« tool_callsï¼‰
        generated: æ­£å‘ç”Ÿæˆçš„ stepsï¼ˆæ¯ä¸ªå…ƒç´ åŒ…å« tool_callsï¼‰

    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«ï¼š
        - total_gt_steps: ground truth æ€»æ­¥æ•°
        - total_gen_steps: generated æ€»æ­¥æ•°
        - step_matches: æ¯ä¸ª step çš„å‡½æ•°é›†åˆå®Œå…¨åŒ¹é…çš„æ•°é‡
        - step_details: æ¯ä¸ª step çš„è¯¦ç»†å¯¹æ¯”ä¿¡æ¯
        - overall_accuracy: æ€»ä½“æ­¥éª¤åŒ¹é…å‡†ç¡®ç‡
    """
    metrics = {
        "total_gt_steps": len(ground_truth),
        "total_gen_steps": len(generated),
        "step_matches": 0,
        "step_details": [],
    }

    # é€æ­¥å¯¹æ¯”
    max_steps = max(len(ground_truth), len(generated))

    for step_idx in range(max_steps):
        step_detail = {
            "step_num": step_idx + 1,
            "gt_functions": [],
            "gen_functions": [],
            "match": False,
            "missing_functions": [],  # åœ¨ GT ä¸­æœ‰ä½† generated ä¸­ç¼ºå¤±
            "extra_functions": [],     # generated ä¸­æœ‰ä½† GT ä¸­æ²¡æœ‰
        }

        # æå– ground truth çš„å‡½æ•°å
        if step_idx < len(ground_truth):
            gt_step = ground_truth[step_idx]
            tool_calls = gt_step.get("tool_calls", [])
            for tc in tool_calls:
                if isinstance(tc, dict):
                    func_name = tc.get("function", "")
                    if func_name:
                        step_detail["gt_functions"].append(func_name)

        # æå– generated çš„å‡½æ•°å
        if step_idx < len(generated):
            gen_step = generated[step_idx]
            tool_calls = gen_step.get("tool_calls", [])
            for tc in tool_calls:
                if isinstance(tc, dict):
                    func_name = tc.get("function", "")
                    if func_name:
                        step_detail["gen_functions"].append(func_name)

        # è½¬æ¢ä¸ºé›†åˆè¿›è¡Œæ¯”è¾ƒ
        gt_funcs_set = set(step_detail["gt_functions"])
        gen_funcs_set = set(step_detail["gen_functions"])

        # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…
        if gt_funcs_set == gen_funcs_set:
            step_detail["match"] = True
            metrics["step_matches"] += 1
        else:
            # æ‰¾å‡ºç¼ºå¤±å’Œå¤šä½™çš„å‡½æ•°
            step_detail["missing_functions"] = list(gt_funcs_set - gen_funcs_set)
            step_detail["extra_functions"] = list(gen_funcs_set - gt_funcs_set)

        metrics["step_details"].append(step_detail)

    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    if metrics["total_gt_steps"] > 0:
        metrics["overall_accuracy"] = metrics["step_matches"] / metrics["total_gt_steps"]
    else:
        metrics["overall_accuracy"] = 0.0

    # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆæ‰€æœ‰æ­¥éª¤éƒ½åŒ¹é…ï¼‰
    metrics["exact_match"] = (
        metrics["total_gt_steps"] == metrics["total_gen_steps"] and
        metrics["step_matches"] == metrics["total_gt_steps"]
    )

    return metrics


async def process_single_record_v1(
    record: Dict[str, Any]
) -> Dict[str, Any]:
    """
    å¤„ç†å•æ¡è®°å½•ï¼šä½¿ç”¨ atomic queries è¿›è¡Œå¤šè½®å¯¹è¯çš„æ­£å‘ rollout å¹¶å¯¹æ¯”

    è¿™æ˜¯ä¸€ä¸ª multi-turn multi-step çš„æµç¨‹ï¼š
    - æ¯ä¸ª atomic_query æ˜¯ä¸€è½®å¯¹è¯ï¼ˆturnï¼‰
    - æ¯è½®å¯¹è¯å†…éƒ¨æ˜¯ multi-step çš„ï¼ˆsingle turn multi-stepï¼‰
    - conversation_history åœ¨æ‰€æœ‰è½®æ¬¡ä¹‹é—´æŒç»­ç´¯åŠ 

    Args:
        record: backward_to_query ç”Ÿæˆçš„è®°å½•

    Returns:
        åŒ…å«å¯¹æ¯”ç»“æœçš„å­—å…¸
    """
    # å¦‚æœè®°å½•æœ¬èº«å°±æœ‰é”™è¯¯ï¼Œç›´æ¥è¿”å›
    if "error" in record:
        return {
            "path_info": record.get("path_info", {}),
            "skipped": True,
            "reason": "original_error",
            "original_error": record["error"]
        }

    atomic_queries = record.get("atomic_queries", [])
    nodes_tool_schema = record.get("nodes_tool_schema", {})
    ground_truth_fc = record.get("fc_results", [])

    if not atomic_queries or not nodes_tool_schema:
        return {
            "path_info": record.get("path_info", {}),
            "skipped": True,
            "reason": "missing_data"
        }

    # æ„å»º OpenAI API æ ¼å¼çš„ toolsï¼ˆåŒ…å«åç§°æ˜ å°„ï¼‰
    tools, name_mapping, short_tool_schemas = build_tools_for_api(nodes_tool_schema)

    all_steps = []
    total_token_usage = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0,
    }

    # åˆå§‹åŒ–å¯¹è¯å†å²ï¼ˆåœ¨æ‰€æœ‰ turns ä¹‹é—´å…±äº«ï¼‰
    conversation_history = [
        {
            "role": "system",
            "content": (
                "You are a multi-turn multi-step function-calling agent. "
                "Before each function call, briefly explain your reasoning: "
                "what you plan to do, which tool to use and why, and how you determine the parameter values. "
                "Express your thinking in a natural, flowing way (1-3 sentences total, not bullet points). "
                "After each step, you will receive the tool outputs and continue until the task is complete. "
                "Always use English."
            )
        }
    ]

    # å¯¹æ¯ä¸ª atomic query è¿›è¡Œä¸€è½®å¯¹è¯
    for turn_idx, atomic_query in enumerate(atomic_queries, start=1):
        print(f"  Processing turn {turn_idx}/{len(atomic_queries)}: {atomic_query[:50]}...")

        # æ·»åŠ å½“å‰è½®æ¬¡çš„ user message
        conversation_history.append({
            "role": "user",
            "content": atomic_query
        })

        # å¤šæ­¥æ‰§è¡Œå¾ªç¯ï¼ˆå½“å‰ turnï¼‰
        turn_completed = False
        max_steps_per_turn = 10

        for step_num in range(1, max_steps_per_turn + 1):
            try:
                # è°ƒç”¨ LLM ç”Ÿæˆå½“å‰æ­¥éª¤çš„å‡½æ•°è°ƒç”¨
                completion = await async_client.chat.completions.create(
                    model=DEFAULT_MODEL,
                    messages=conversation_history,
                    tools=tools,
                    stream=False,
                    temperature=1,
                    max_completion_tokens=1024,
                )

                message = completion.choices[0].message

                # ç´¯è®¡ token ä½¿ç”¨
                usage = completion.usage
                if usage:
                    total_token_usage["prompt_tokens"] += usage.prompt_tokens
                    total_token_usage["completion_tokens"] += usage.completion_tokens
                    total_token_usage["total_tokens"] += usage.total_tokens

                # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
                if not message.tool_calls:
                    # æ²¡æœ‰ tool callsï¼Œå½“å‰ turn å®Œæˆ
                    final_step = {
                        "turn_idx": turn_idx,
                        "step_num": len(all_steps) + 1,
                        "summary": message.content or "",
                        "tool_calls": [],
                        "tool_outputs": []
                    }
                    all_steps.append(final_step)

                    # å°†æœ€åçš„ assistant æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
                    conversation_history.append({
                        "role": "assistant",
                        "content": message.content or ""
                    })

                    print(f"    Turn {turn_idx} completed at step {step_num}")
                    turn_completed = True
                    break

                # æå– tool_calls
                api_tool_calls = message.tool_calls

                # è®°å½•å½“å‰æ­¥éª¤
                tool_calls_for_record = []
                for tc in api_tool_calls:
                    try:
                        params = json.loads(tc.function.arguments) if tc.function.arguments else {}
                    except json.JSONDecodeError:
                        params = {}

                    tool_calls_for_record.append({
                        "function": tc.function.name,
                        "parameters": params
                    })

                current_step = {
                    "turn_idx": turn_idx,
                    "step_num": len(all_steps) + 1,
                    "tool_calls": tool_calls_for_record,
                    "tool_outputs": []
                }

                # æ‰§è¡Œå‡½æ•°è°ƒç”¨
                step_outputs = []
                tool_messages = []

                for tc in api_tool_calls:
                    short_func_name = tc.function.name
                    tool_call_id = tc.id

                    try:
                        parameters = json.loads(tc.function.arguments) if tc.function.arguments else {}
                    except json.JSONDecodeError:
                        parameters = {}

                    try:
                        # æ˜ å°„å›åŸå§‹åç§°æ‰§è¡Œ
                        original_func_name = name_mapping.get(short_func_name, short_func_name)
                        output_result = await execute_function_call(original_func_name, parameters)

                        # æå–å®é™…çš„ result
                        if isinstance(output_result, dict):
                            # å¦‚æœæœ‰ "result" å­—æ®µï¼Œæå–å®ƒï¼›å¦åˆ™ä½¿ç”¨æ•´ä¸ª dict
                            if "result" in output_result:
                                output = output_result["result"]
                            else:
                                # æ²¡æœ‰ "result" å­—æ®µï¼Œä½¿ç”¨æ•´ä¸ª dict ä½†ç§»é™¤ token_usage
                                output = {k: v for k, v in output_result.items() if k != "token_usage"}

                            # ç´¯åŠ  execute_function_call çš„ token ä½¿ç”¨
                            tq = output_result.get("token_usage", {})
                            total_token_usage["prompt_tokens"] += tq.get("prompt_tokens", 0)
                            total_token_usage["completion_tokens"] += tq.get("completion_tokens", 0)
                            total_token_usage["total_tokens"] += tq.get("total_tokens", 0)
                        else:
                            output = output_result

                        step_outputs.append({
                            "function": short_func_name,  # ä½¿ç”¨ç¼©çŸ­çš„åç§°
                            "output": output
                        })

                        # æ„å»º tool message
                        tool_output_str = json.dumps(output, ensure_ascii=False) if isinstance(output, (dict, list)) else str(output)
                        tool_messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call_id,
                            "content": tool_output_str
                        })

                    except Exception as e:
                        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                        raise RuntimeError(
                            f"Error executing function {short_func_name} with parameters {parameters} at turn {turn_idx}, step {step_num}: {e}"
                        ) from e

                current_step["tool_outputs"] = step_outputs
                all_steps.append(current_step)

                # å°† assistant messageï¼ˆå¸¦ tool_callsï¼‰æ·»åŠ åˆ°å¯¹è¯å†å²
                conversation_history.append({
                    "role": "assistant",
                    "content": message.content or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in api_tool_calls
                    ]
                })

                # æ·»åŠ æ‰€æœ‰ tool messages
                conversation_history.extend(tool_messages)

            except Exception as e:
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                raise RuntimeError(
                    f"Error at turn {turn_idx}, step {step_num}: {e}"
                ) from e

        if not turn_completed:
            print(f"    Turn {turn_idx} reached max steps ({max_steps_per_turn})")

    # å¯¹æ¯”ç»“æœï¼ˆä½¿ç”¨ v1 ç‰ˆæœ¬çš„æ¯”è¾ƒå‡½æ•°ï¼ŒæŒ‰ turn æ¯”è¾ƒï¼‰
    metrics = compare_function_calls_v1(ground_truth_fc, all_steps)

    # æ„å»º tools åˆ—è¡¨ï¼šä½¿ç”¨ç¼©çŸ­åç§°çš„ schemas
    tools_list = [
        {"function_schema": tool_meta.get("function_schema", {})}
        for tool_meta in short_tool_schemas.values()
        if "function_schema" in tool_meta
    ]

    return {
        "path_info": record.get("path_info", {}),
        "atomic_queries": atomic_queries,
        "total_turns": len(atomic_queries),
        "ground_truth_steps": len(ground_truth_fc),
        "generated_steps": len(all_steps),
        "metrics": metrics,
        "token_usage": total_token_usage,
        "ground_truth_fc": ground_truth_fc,
        "generated_fc": all_steps,
        "conversation": conversation_history,
        "tools": tools_list,
        "tool_name_mapping": name_mapping
    }


async def process_single_record(
    record: Dict[str, Any]
) -> Dict[str, Any]:
    """
    å¤„ç†å•æ¡è®°å½•ï¼šæ­£å‘ rollout å¹¶å¯¹æ¯”

    Args:
        record: backward_to_query ç”Ÿæˆçš„è®°å½•

    Returns:
        åŒ…å«å¯¹æ¯”ç»“æœçš„å­—å…¸
    """
    # å¦‚æœè®°å½•æœ¬èº«å°±æœ‰é”™è¯¯ï¼Œç›´æ¥è¿”å›
    if "error" in record:
        return {
            "path_info": record.get("path_info", {}),
            "skipped": True,
            "reason": "original_error",
            "original_error": record["error"]
        }

    final_query = record.get("final_query", "")
    nodes_tool_schema = record.get("nodes_tool_schema", {})
    ground_truth_fc = record.get("fc_results", [])

    if not final_query or not nodes_tool_schema:
        return {
            "path_info": record.get("path_info", {}),
            "skipped": True,
            "reason": "missing_data"
        }

    # æ­£å‘ rolloutï¼ˆå¤šæ­¥æ‰§è¡Œï¼‰
    rollout_result = await forward_rollout_step(
        user_query=final_query,
        tool_schemas=nodes_tool_schema,
        conversation_history=[],
        max_steps=10
    )

    if "error" in rollout_result:
        return {
            "path_info": record.get("path_info", {}),
            "final_query": final_query,
            "rollout_error": rollout_result.get("error"),
            "token_usage": rollout_result.get("token_usage", {}),
        }

    # å¯¹æ¯”ç»“æœ
    generated_steps = rollout_result.get("steps", [])
    metrics = compare_function_calls(ground_truth_fc, generated_steps)
    conversation_history = rollout_result["conversation_history"]

    # è·å–ç¼©çŸ­åç§°åçš„ tool schemas
    short_tool_schemas = rollout_result.get("short_tool_schemas", {})
    name_mapping = rollout_result.get("tool_name_mapping", {})

    # æ„å»º tools åˆ—è¡¨ï¼šä½¿ç”¨ç¼©çŸ­åç§°çš„ schemas
    tools_list = [
        {"function_schema": tool_meta.get("function_schema", {})}
        for tool_meta in short_tool_schemas.values()
        if "function_schema" in tool_meta
    ]

    return {
        "path_info": record.get("path_info", {}),
        "final_query": final_query,
        "ground_truth_steps": len(ground_truth_fc),
        "generated_steps": len(generated_steps),
        "metrics": metrics,
        "token_usage": rollout_result.get("token_usage", {}),
        "ground_truth_fc": ground_truth_fc,
        "generated_fc": generated_steps,
        "conversation": conversation_history,
        "tools": tools_list,
        "tool_name_mapping": name_mapping
    }


async def run_distillation(
    max_records: Optional[int] = None,
    batch_size: int = 5,
    use_atomic_queries: bool = False,
    resume: bool = False,
    early_stop_batches: int = 3,
) -> None:
    """
    è¿è¡Œæ­£å‘è’¸é¦éªŒè¯

    Args:
        max_records: æœ€å¤šå¤„ç†çš„è®°å½•æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
        use_atomic_queries: æ˜¯å¦ä½¿ç”¨ atomic queries è¿›è¡Œå¤šè½®å¯¹è¯
                           False: ä½¿ç”¨ final_query çš„å•è½®å¤šæ­¥å¯¹è¯ï¼ˆprocess_single_recordï¼‰
                           True: ä½¿ç”¨ atomic_queries çš„å¤šè½®å¤šæ­¥å¯¹è¯ï¼ˆprocess_single_record_v1ï¼‰
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²æˆåŠŸå¤„ç†çš„è®°å½•ï¼‰
        early_stop_batches: è¿ç»­å¤šå°‘ä¸ª batch å…¨éƒ¨å¤±è´¥ååœæ­¢ï¼ˆ0 è¡¨ç¤ºä¸å¯ç”¨æ—©åœï¼‰
    """
    print(f"Loading backward queries from {BACKWARD_QUERIES_PATH}...")

    records = []
    with open(BACKWARD_QUERIES_PATH, "r", encoding="utf-8") as f:
        for line in f:
            records.append(json.loads(line))
            if max_records and len(records) >= max_records:
                break

    print(f"Loaded {len(records)} records.")

    # æ ¹æ®ç‰ˆæœ¬é€‰æ‹©è¾“å‡ºè·¯å¾„
    if use_atomic_queries:
        output_path = OUTPUT_DISTILL_RESULTS_PATH.replace(".jsonl", "_multi_turn.jsonl")
        print(f"Mode: Multi-turn (using atomic queries)")
    else:
        output_path = OUTPUT_DISTILL_RESULTS_PATH
        print(f"Mode: Single-turn (using final query)")

    print(f"Output -> {output_path}\n")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # æ–­ç‚¹ç»­ä¼ ï¼šè¯»å–å·²æˆåŠŸå¤„ç†çš„è®°å½•
    successfully_processed_ids = set()

    if resume and os.path.exists(output_path):
        print(f"\nğŸ”„ Resume mode enabled, reading existing results from {output_path}...")
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line.strip())
                        # åªç»Ÿè®¡æˆåŠŸå¤„ç†ä¸”æœ‰ metrics çš„è®°å½•
                        if not record.get("skipped") and not record.get("error") and "metrics" in record:
                            path_info = record.get("path_info", {})
                            # ä½¿ç”¨ start_index å’Œ walk_id ä½œä¸ºå”¯ä¸€æ ‡è¯†
                            start_idx = path_info.get("start_index")
                            walk_id = path_info.get("walk_id")
                            if start_idx is not None and walk_id is not None:
                                record_id = (start_idx, walk_id)
                                successfully_processed_ids.add(record_id)
                    except json.JSONDecodeError:
                        continue

            print(f"   Found {len(successfully_processed_ids)} successfully processed records")

            # è¿‡æ»¤æ‰æˆåŠŸå¤„ç†çš„è®°å½•
            original_count = len(records)
            records = [
                r for r in records
                if not (
                    r.get("path_info", {}).get("start_index") is not None and
                    r.get("path_info", {}).get("walk_id") is not None and
                    (r.get("path_info", {}).get("start_index"), r.get("path_info", {}).get("walk_id")) in successfully_processed_ids
                )
            ]
            skipped_count = original_count - len(records)

            print(f"   Skipping {skipped_count} successfully processed records")
            print(f"   Remaining {len(records)} records to process")

            if len(records) == 0:
                print("\nâœ… All records already processed! Nothing to do.")
                return

        except Exception as e:
            print(f"âš ï¸  Warning: Failed to read existing results: {e}")
            print("   Continuing without resume...")
            successfully_processed_ids.clear()

    # æ–‡ä»¶å†™å…¥æ¨¡å¼ï¼šresume ä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼Œå¦åˆ™ä½¿ç”¨è¦†ç›–æ¨¡å¼
    file_mode = "a" if resume else "w"
    if file_mode == "a":
        print(f"ğŸ“ Appending to existing file: {output_path}\n")

    total_tokens = 0
    total_exact_matches = 0
    total_processed = 0
    total_errors = 0
    consecutive_failed_batches = 0  # è¿ç»­å¤±è´¥çš„ batch è®¡æ•°
    total_steps = 0  # ç´¯ç§¯æ€»æ­¥æ•°
    total_turns = 0  # ç´¯ç§¯æ€»è½®æ¬¡æ•°

    # é€‰æ‹©ä½¿ç”¨å“ªä¸ªå¤„ç†å‡½æ•°
    process_func = process_single_record_v1 if use_atomic_queries else process_single_record

    with open(output_path, file_mode, encoding="utf-8") as f:
        num_batches = (len(records) + batch_size - 1) // batch_size

        for batch_idx, start in enumerate(tqdm(range(0, len(records), batch_size),
                                               total=num_batches,
                                               desc="Processing records",
                                               unit="batch"), start=1):
            batch = records[start: start + batch_size]
            print(f"\n[Batch {batch_idx}/{num_batches}] Processing records {start + 1}-{start + len(batch)}...")

            batch_errors = 0  # å½“å‰ batch çš„é”™è¯¯æ•°
            batch_tokens = 0
            batch_steps_per_turn = []  # å½“å‰ batch çš„æ¯ä¸ªturnå¹³å‡æ­¥æ•°åˆ—è¡¨

            tasks = [process_func(record) for record in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, Exception):
                    print(f"[ERROR] Exception: {result}")
                    batch_errors += 1
                    total_errors += 1
                    # å¤±è´¥çš„è®°å½•ä¸å†™å…¥æ–‡ä»¶ï¼Œä¸‹æ¬¡ resume ä¼šé‡æ–°å¤„ç†
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯å¤„ç†å¤±è´¥ï¼ˆæœ‰ error æˆ– rollout_errorï¼‰
                if result.get("error") or result.get("rollout_error"):
                    batch_errors += 1
                    total_errors += 1
                    # å¤±è´¥çš„è®°å½•ä¸å†™å…¥æ–‡ä»¶
                    continue

                # åªå†™å…¥æˆåŠŸçš„è®°å½•
                f.write(json.dumps(result, ensure_ascii=False) + "\n")
                f.flush()

                # ç»Ÿè®¡
                if not result.get("skipped") and "metrics" in result:
                    total_processed += 1
                    if result["metrics"].get("exact_match"):
                        total_exact_matches += 1

                    # ç´¯ç§¯æ€»æ­¥æ•°å’Œæ€»è½®æ¬¡æ•°
                    generated_steps = result.get("generated_steps", 0)
                    record_turns = result.get("total_turns", 1)  # å•è½®æ¨¡å¼é»˜è®¤ä¸º1
                    total_steps += generated_steps
                    total_turns += record_turns

                    # è®¡ç®—å½“å‰è®°å½•çš„å¹³å‡æ¯ä¸ªturnçš„æ­¥æ•°
                    steps_per_turn = generated_steps / record_turns if record_turns > 0 else 0
                    batch_steps_per_turn.append(steps_per_turn)

                token_usage = result.get("token_usage", {})
                batch_tokens += token_usage.get("total_tokens", 0)

            total_tokens += batch_tokens

            # è®¡ç®—å½“å‰ batch çš„å¹³å‡æ¯ä¸ªturnçš„æ­¥æ•°
            avg_steps_per_turn = sum(batch_steps_per_turn) / len(batch_steps_per_turn) if batch_steps_per_turn else 0

            # æ—©åœæ£€æŸ¥ï¼šå¦‚æœå½“å‰ batch å…¨éƒ¨å¤±è´¥
            if batch_errors == len(batch):
                consecutive_failed_batches += 1
                print(
                    f"[Batch {batch_idx}] âŒ ALL {len(batch)} records FAILED! "
                    f"(consecutive failed batches: {consecutive_failed_batches}/{early_stop_batches})"
                )

                # å¦‚æœè¿ç»­å¤±è´¥çš„ batch æ•°é‡è¾¾åˆ°é˜ˆå€¼ï¼Œåœæ­¢å¤„ç†
                if early_stop_batches > 0 and consecutive_failed_batches >= early_stop_batches:
                    print("\n" + "=" * 80)
                    print("ğŸ›‘ EARLY STOPPING TRIGGERED")
                    print("=" * 80)
                    print(f"Consecutive failed batches: {consecutive_failed_batches}")
                    print(f"Stopping to prevent further failures...")
                    print("=" * 80)
                    break
            else:
                # å¦‚æœå½“å‰ batch æœ‰æˆåŠŸçš„ï¼Œé‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                consecutive_failed_batches = 0
                print(
                    f"[Batch {batch_idx}] batch_tokens={batch_tokens}, "
                    f"overall_tokens={total_tokens}, "
                    f"batch_errors={batch_errors}/{len(batch)}, "
                    f"avg_steps_per_turn={avg_steps_per_turn:.2f}"
                )

    print("\n" + "=" * 80)
    print(f"DISTILLATION SUMMARY ({'Multi-turn' if use_atomic_queries else 'Single-turn'})")
    print("=" * 80)
    print(f"Total records: {len(records)}")
    print(f"Processed: {total_processed}")
    print(f"Failed: {total_errors}")
    print(f"Exact matches: {total_exact_matches}")
    if total_processed > 0:
        print(f"Exact match rate: {total_exact_matches / total_processed * 100:.2f}%")
    print(f"Total tokens used: {total_tokens}")
    if total_turns > 0:
        print(f"Average steps per turn: {total_steps / total_turns:.2f}")
    print("=" * 80)
    print(f"\nResults saved to: {output_path}")


def main() -> None:
    """
    å‘½ä»¤è¡Œå…¥å£
    """
    import argparse

    parser = argparse.ArgumentParser(description="Run positive distillation validation")
    parser.add_argument('--max-records', type=int, default=None,
                        help='Maximum number of records to process (for testing)')
    parser.add_argument('--batch-size', type=int, default=5,
                        help='Batch size for parallel processing')
    parser.add_argument('--multi-turn', '-m', action='store_true',
                        help='Use multi-turn mode (atomic queries)')
    parser.add_argument('--resume', action='store_true',
                        help='Resume from previous run (skip already processed records)')
    parser.add_argument('--early-stop', type=int, default=1,
                        help='Stop after N consecutive batches with all failures (0 to disable)')
    parser.add_argument('--test', action='store_true',
                        help='Test mode: process only 5 records')

    args = parser.parse_args()

    if args.test:
        print("ğŸ§ª Running in TEST mode (5 records only)...")
        args.max_records = 5

    use_atomic_queries = args.multi_turn

    if use_atomic_queries:
        print("Running multi-turn version (using atomic queries)...")
    else:
        print("Running single-turn version (using final query)...")

    asyncio.run(run_distillation(
        max_records=args.max_records,
        batch_size=args.batch_size,
        use_atomic_queries=use_atomic_queries,
        resume=args.resume,
        early_stop_batches=args.early_stop,
    ))


if __name__ == "__main__":
    main()
